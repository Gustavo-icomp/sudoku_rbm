{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099dd69e-241b-475d-a87b-2324be809fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0445284d-f24e-4aff-bc40-17a7724c5ed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'torch (Python -1.-1.-1)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "%pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92da7d5-fc4e-407f-baa9-279534200b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Valid Sudoku Data\n",
    "# We’ll generate valid 3x3 Sudoku puzzles for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30eb6abd-7f34-4e55-aa83-3326b6f75363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sudoku data shape: (1000, 27)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_valid_sudoku_grids(num_samples):\n",
    "    \"\"\"\n",
    "    Generate valid 3x3 Sudoku grids.\n",
    "    Each cell is represented as a one-hot encoded vector.\n",
    "    \"\"\"\n",
    "    grids = []\n",
    "    for _ in range(num_samples):\n",
    "        # Create a valid 3x3 Sudoku grid\n",
    "        grid = np.array([[1, 2, 3],\n",
    "                         [3, 1, 2],\n",
    "                         [2, 3, 1]])\n",
    "        # Randomly permute rows and columns to generate variations\n",
    "        grid = grid[np.random.permutation(3)][:, np.random.permutation(3)]\n",
    "        # Flatten the grid and one-hot encode\n",
    "        flattened = grid.flatten() - 1  # Convert to 0-based index\n",
    "        one_hot = np.eye(3)[flattened].flatten()\n",
    "        grids.append(one_hot)\n",
    "    return np.array(grids)\n",
    "\n",
    "# Generate 1000 valid Sudoku grids\n",
    "sudoku_data = generate_valid_sudoku_grids(1000)\n",
    "print(\"Sudoku data shape:\", sudoku_data.shape)  # Should be (1000, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c15e2-587f-4b9a-94e7-0907b223903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define and Train the First RBM\n",
    "# We’ll use TensorFlow to define and train the RBMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1449c2e2-0ac6-48c6-8086-6aa4bf1b26e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute MatMul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:MatMul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Define and train the first RBM\u001b[39;00m\n\u001b[1;32m     53\u001b[0m rbm1 \u001b[38;5;241m=\u001b[39m RBM(visible_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m27\u001b[39m, hidden_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m rbm1\u001b[38;5;241m.\u001b[39mtrain(sudoku_data, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mRBM.train\u001b[0;34m(self, data, epochs, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m batch \u001b[38;5;241m=\u001b[39m data[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Positive phase\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     h_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_hidden(batch)\n\u001b[1;32m     25\u001b[0m     positive_grad \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(tf\u001b[38;5;241m.\u001b[39mtranspose(batch), h_prob)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Negative phase\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 44\u001b[0m, in \u001b[0;36mRBM.sample_hidden\u001b[0;34m(self, visible)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_hidden\u001b[39m(\u001b[38;5;28mself\u001b[39m, visible):\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sample hidden units given visible units.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     h_prob \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msigmoid(tf\u001b[38;5;241m.\u001b[39mmatmul(visible, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_bias)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(tf\u001b[38;5;241m.\u001b[39msign(h_prob \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(tf\u001b[38;5;241m.\u001b[39mshape(h_prob))))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute MatMul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:MatMul] name: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, visible_units, hidden_units, learning_rate=0.01):\n",
    "        self.visible_units = visible_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W = tf.Variable(tf.random.normal([visible_units, hidden_units], stddev=0.1))\n",
    "        self.v_bias = tf.Variable(tf.zeros([visible_units]))\n",
    "        self.h_bias = tf.Variable(tf.zeros([hidden_units]))\n",
    "\n",
    "    def train(self, data, epochs=10, batch_size=32):\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i + batch_size]\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Positive phase\n",
    "                    h_prob = self.sample_hidden(batch)\n",
    "                    positive_grad = tf.matmul(tf.transpose(batch), h_prob)\n",
    "\n",
    "                    # Negative phase\n",
    "                    v_recon = self.sample_visible(h_prob)\n",
    "                    h_recon_prob = self.sample_hidden(v_recon)\n",
    "                    negative_grad = tf.matmul(tf.transpose(v_recon), h_recon_prob)\n",
    "\n",
    "                    # Compute gradients and update weights\n",
    "                    delta_W = (positive_grad - negative_grad) / tf.cast(tf.shape(batch)[0], tf.float32)\n",
    "                    delta_v_bias = tf.reduce_mean(batch - v_recon, axis=0)\n",
    "                    delta_h_bias = tf.reduce_mean(h_prob - h_recon_prob, axis=0)\n",
    "\n",
    "                    grads = [delta_W, delta_v_bias, delta_h_bias]\n",
    "                    optimizer.apply_gradients(zip(grads, [self.W, self.v_bias, self.h_bias]))\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "\n",
    "    def sample_hidden(self, visible):\n",
    "        \"\"\"Sample hidden units given visible units.\"\"\"\n",
    "        h_prob = tf.sigmoid(tf.matmul(visible, self.W) + self.h_bias)\n",
    "        return tf.nn.relu(tf.sign(h_prob - tf.random.uniform(tf.shape(h_prob))))\n",
    "\n",
    "    def sample_visible(self, hidden):\n",
    "        \"\"\"Sample visible units given hidden units.\"\"\"\n",
    "        v_prob = tf.sigmoid(tf.matmul(hidden, tf.transpose(self.W)) + self.v_bias)\n",
    "        return tf.nn.relu(tf.sign(v_prob - tf.random.uniform(tf.shape(v_prob))))\n",
    "\n",
    "# Define and train the first RBM\n",
    "rbm1 = RBM(visible_units=27, hidden_units=20, learning_rate=0.01)\n",
    "rbm1.train(sudoku_data, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e1429-b1ed-47c1-b11e-3f8ad031dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Second RBM\n",
    "# Use the hidden layer activations from the first RBM as input to the second RBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c4e83-58e2-4632-a2b6-2524cea66b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden layer activations from the first RBM\n",
    "hidden_activations_1 = rbm1.sample_hidden(sudoku_data)\n",
    "\n",
    "# Define and train the second RBM\n",
    "rbm2 = RBM(visible_units=20, hidden_units=10, learning_rate=0.01)\n",
    "rbm2.train(hidden_activations_1, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2d60d-88e2-434b-a5f3-64ad065f79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Fine-Tune the Network (Optional)\n",
    "# If you have labeled data, you can fine-tune the network using supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcde57-60ef-4569-8cca-c0eddd433969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a softmax layer for classification\n",
    "inputs = Input(shape=(10,))  # Output of the second RBM\n",
    "outputs = Dense(3, activation='softmax')(inputs)  # 3 classes (1, 2, 3)\n",
    "classifier = Model(inputs, outputs)\n",
    "\n",
    "# Compile the classifier\n",
    "classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the network (assuming labeled_data is available)\n",
    "# labeled_data = (X_train, y_train)\n",
    "# classifier.fit(X_train, y_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4fe68-a6d5-4994-a4f7-32b26b622cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Generate or Complete Sudoku Puzzles\n",
    "# Use the trained RBMs to generate new Sudoku puzzles or complete partially filled ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df66691-10e0-4e35-9bf7-92aa749789eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sudoku(rbm1, rbm2):\n",
    "    \"\"\"Generate a new Sudoku grid using the trained RBMs.\"\"\"\n",
    "    # Start with random visible units\n",
    "    visible = tf.random.uniform([1, 27])\n",
    "    # Sample through the RBMs\n",
    "    hidden1 = rbm1.sample_hidden(visible)\n",
    "    hidden2 = rbm2.sample_hidden(hidden1)\n",
    "    visible_recon = rbm1.sample_visible(rbm2.sample_visible(hidden2))\n",
    "    # Reshape and decode the one-hot encoded grid\n",
    "    grid = np.argmax(visible_recon.numpy().reshape(3, 3, 3), axis=2) + 1\n",
    "    return grid\n",
    "\n",
    "def complete_sudoku(rbm1, rbm2, partial_grid):\n",
    "    \"\"\"Complete a partially filled Sudoku grid.\"\"\"\n",
    "    # Flatten and one-hot encode the partial grid\n",
    "    partial_flattened = (partial_grid.flatten() - 1).astype(int)\n",
    "    partial_one_hot = np.eye(3)[partial_flattened].flatten()\n",
    "    # Sample through the RBMs\n",
    "    hidden1 = rbm1.sample_hidden(partial_one_hot[np.newaxis, :])\n",
    "    hidden2 = rbm2.sample_hidden(hidden1)\n",
    "    visible_recon = rbm1.sample_visible(rbm2.sample_visible(hidden2))\n",
    "    # Reshape and decode the one-hot encoded grid\n",
    "    completed_grid = np.argmax(visible_recon.numpy().reshape(3, 3, 3), axis=2) + 1\n",
    "    return completed_grid\n",
    "\n",
    "# Example usage\n",
    "new_sudoku = generate_sudoku(rbm1, rbm2)\n",
    "print(\"Generated Sudoku:\\n\", new_sudoku)\n",
    "\n",
    "partial_grid = np.array([[1, 0, 0],\n",
    "                         [0, 0, 0],\n",
    "                         [0, 0, 0]])  # 0 represents empty cells\n",
    "completed_sudoku = complete_sudoku(rbm1, rbm2, partial_grid)\n",
    "print(\"Completed Sudoku:\\n\", completed_sudoku)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
